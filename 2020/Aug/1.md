# August 1st 2020

Today I'm continuing my quest in
data analysis. I'm going through
the freecodecamp data analysis
course. Currently checking out
some videos.

The current videos are something
that I already know most about.
Thinking if I should still see them.

One new thing that I learned is
there's a free service called
Notebooks that can host jupyter
lab, which has jupyter notebook.

Till date, I have come across the
following tools or services to help
with Jupyter Notebooks
* Local Jupyter Notebook
* Local Jupyter Lab which contains
Jupyter Notebook along with other
things
* Google Colab - which contains
Jupyter Notebook along with
collaboration - people can comment
on cells, people can execute cells
and it will show who last executed
a cell. It also provides a table of
contents on the left based on 
headings and it provides a way to
link to the different headings and
scroll to them. It contains all the
features of Jupyter notebook too,
I believe. Colab seems to be free.
I tried it only in my company
google account though. I noticed that
it connects to a Cloud Python 3
runtime, and it was inside a Google
Compute Engine. It shows stats about
the machine - like RAM, CPU. Since
Colab is a collaborative thing,
it has versions and stores the
cell output, along with who
executed last. Sometimes I have
noticed it to have save issues,
showing merge conflicts - usually
in real time collaborative editing,
automatic merge conflict resolution
is done, and that's the beauty of it.
I think here it was kind of tough,
similar to how it's hard to do
automatic in git in some cases. So,
Colab shows the diff to the user
and asks the user to save the 
changes.
* Notebooks - https://notebooks.ai 
seems to be an "always free, free
forever" tool that hosts Jupyter
Lab in the cloud. I noticed that
the limiting thing is storage.
They provide 1GB storage by default.
That's still a lot. Also, by inviting
friends, we can get more, upto 3GB
it seems. And they have some pricing
plans for more, but I'm not able to
access their pricing page at all
for some reason
* Jupyter Hub - https://github.com/jupyterhub
	* https://jupyter.org/hub
	* https://github.com/jupyterhub/jupyterhub
* Binder - https://mybinder.org/
	https://mybinder.readthedocs.io/en/latest/
	* BinderHub - https://github.com/jupyterhub/binderhub
	https://binderhub.readthedocs.io


Jupyter notebooks can have
* Markdown
* Python Code

Each of these are put in cells.
Everything is put inside cells it
seems. Each cell is either a
code cell or text (markdown) cell

Apparently the code can be any
programming language and not
just python! I noticed notebooks.ai
mentioning about R, Julia and Python
code!

I can also see Juypter notebook
website - https://jupyter.org/
showing so many languages. I don't
know if it's possible, but it does
show many programming languages,
including compiled ones too,
like golang! But I couldn't find
anything in the documentation.
Maybe I'll check it out later!

Okay, I just checked a bit
now :P There was this thing
called Kernels in Jupyter
docs - https://jupyter.org/documentation

I can see "IPython", "IRKernel",
"IJulia". IPython is short for
Interactive Python. Same for R
and Julia. And they are called
"Kernels".

There's a kernel for Golang.
I found one over here - 
https://github.com/gopherdata/gophernotes
There might be more for golang 🤷‍♂️ 

I just found out another tool
by lurking around
https://nteract.io/

nteract was mentioned in
gophernotes golang kernel, which
is a golang kernel for BOTH
Jupyter notebooks AND nteract

gophernotes seems cool! Something
to use when teaching golang
may be

Under the hood gophernotes uses
gomacro - https://github.com/cosmos72/gomacro 
as the Golang interpreter it seems

nteract seems like something
similar to Jupyter notebook.
It too supports multiple
languages and has kernels for
them https://nteract.io/kernels

hydrogen - running code
interactively inside atom text
editor! including showing
plots with python code! kind of
like jupyter notebook experience
but in the text editor itself!

https://github.com/nteract/hydrogen

---

Now I got bored with the basic
stuff that the videos were telling.
So I jumped into the projects in
freecodecamp to see how they are,
as I realized almost the whole
course was full of videos
and not any interactive thing which
is a bit boring for me!


So I started off with the first
project

https://www.freecodecamp.org/learn/data-analysis-with-python/data-analysis-with-python-projects/mean-variance-standard-deviation-calculator

It seems to be a very very very
simple problem, using numpy
which I have never used.

Anyways, I forked the repl project
and I started playing with numpy
by doing

```
> import numpy as np
> help(np)
```

And I found that the documentation
can be found at `scipy.org` which
had many links along with
`numpy.org`. From there I looked
through a bit of their homepage,
then moved on to

https://numpy.org/doc/stable/user/quickstart.html

It gave me a head start on the
fact that numpy can do stuff with
n dimensional arrays. Took me a
minute to understand as I kept
assuming it to be 2 dimensional
arrays. n as in, it can be any
dimension. I noticed some
functions

I saw `reshape`, `arange`, and
some basic stuff. Didn't try them
all. Just saw they exist and
read through some of them

I quickly realized that there
was a `max` function which can
help with `maximum`

I started trying things in the
repl.it repl creating the
matrix with `array` and shaping it
with `reshape` to tell the number
of rows and columns, as it's only
2 dimensional

And I tried stuff. Based
on my assumptions, `min` worked
too. Then `variance` didn't
instead there's `var` and `std`
are present and I'm going to
try it out

https://numpy.org/doc/stable/reference/generated/numpy.var.html#numpy.var

https://numpy.org/doc/stable/reference/generated/numpy.std.html#numpy.std

And to these, we can tell which
axis to calculate the value.
If there are n axes, the
`axis` argument values can be
`0` to `n-1`. So, in my case, `0`
and `1`. And `0` referred to axis
1, which is the columns, and the `1`
referred to axis 2, which is the
rows

I also was trying to see if the
shape can be mentioned as an
argument in `array` function
while creating the array. But no,
doesn't seem like it. I mean, I
did find this thing called `ndmin`,
which I first misread as `ndim`.
`ndim` refers to number of axes
or dimensions. `ndmin` is the
minimum number of dimensions or
something and when I gave `2`
for my input, I got some weird
output

```
> a = np.array([0,1,2,3,4,5,6,7,8], ndmin=2)
> a
array([[0, 1, 2, 3, 4, 5, 6, 7, 8]])
```

It created a 2D array, but with
just one list / row.

Anyways, however just mentioning
dimensions doesn't make sense.
Like for a list with 8 elements,
if I saw 2 dimensions, it could be
shaped as 1 x 8, 8 x 1, 4 x 2,
2 x 4 . I guess `reshape` is perfect
for my use case for a 3 x 3 matrix!

Now I'm checking how errors are
thrown, as according to the
project I need to throw error
when I don't have enough input.
Weird they didn't tell the same,
like, throw error when more than 
enough input is given too. Anyways,
I'll check it out now

This seems like a nice
stackoverflow answer

https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python#24065533

And my project clearly mentions
that I need to raise
`ValueError` which is a specific
error apparently.

Okay, I started solving the project.
I didn't raise any errors and just
went ahead and ran the project
to see how the tests were working
and this is what I got

```python
{'mean': [array([3., 4., 5.]), array([1., 4., 7.]), 4.0], 'variance': [array([6., 6., 6.]), array([0.66666667, 0.66666667, 0.66666667]), 6.666666666666667], 'standard deviation': [array([2.44948974, 2.44948974, 2.44948974]), array([0.81649658, 0.81649658, 0.81649658]), 2.581988897471611], 'max': [array([6, 7, 8]), array([2, 5, 8]), 8], 'min': [array([0, 1, 2]), array([0, 3, 6]), 0], 'sum': [array([ 9, 12, 15]), array([ 3, 12, 21]), 36]}
EEF
======================================================================
ERROR: test_calculate (test_module.UnitTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/DodgerblueSimpleSolidstatedrive/test_module.py", line 10, in test_calculate
    self.assertAlmostEqual(actual, expected, "Expected different output when calling 'calculate()' with '[2,6,2,8,4,0,1,5,7]'")
  File "/usr/lib/python3.8/unittest/case.py", line 937, in assertAlmostEqual
    if first == second:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

======================================================================
ERROR: test_calculate2 (test_module.UnitTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/DodgerblueSimpleSolidstatedrive/test_module.py", line 15, in test_calculate2
    self.assertAlmostEqual(actual, expected, "Expected different output when calling 'calculate()' with '[9,1,5,3,3,3,2,9,0]'")
  File "/usr/lib/python3.8/unittest/case.py", line 937, in assertAlmostEqual
    if first == second:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

======================================================================
FAIL: test_calculate_with_few_digits (test_module.UnitTests)
----------------------------------------------------------------------
ValueError: cannot reshape array of size 7 into shape (3,3)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/runner/DodgerblueSimpleSolidstatedrive/test_module.py", line 18, in test_calculate_with_few_digits
    self.assertRaisesRegex(ValueError, "List must contain nine numbers.", mean_var_std.calculate, [2,6,2,8,4,0,1,])
AssertionError: "List must contain nine numbers." does not match "cannot reshape array of size 7 into shape (3,3)"

----------------------------------------------------------------------
Ran 3 tests in 0.039s

FAILED (failures=1, errors=2)
```

I realized that the instructions
clearly said they don't want
Numpy arrays. And all the output
I get might be Numpy arrays and
not python lists / arrays

I guess it kind of makes sense
because the output I see, for 
example

```python
array([3., 4., 5.])
```

I mean, that should have been

```python
[3.0, 4.0, 5.0]
```

I did notice the missing `.0`, but
I overlooked the extra `array`.
I think that's the numpy array

I can also see the same in the
test output and clearly they
don't want that extra `array()`
thingy. I gotta check how to
convert numpy arrays to simple
python arrays or lists

I checked out this stackoverflow
answer

https://stackoverflow.com/questions/1966207/converting-numpy-array-into-python-list-structure#39682665

It mentioned something about
`tolist` and it seems to work!

https://numpy.org/doc/stable/reference/generated/numpy.char.chararray.tolist.html#numpy.char.chararray.tolist

```python
> arr.max(axis=0)
array([6, 7, 8])
> arr.max(axis=0).tolist()
[6, 7, 8]
```

So I'm going to use just that!

```python
{'mean': [[3.0, 4.0, 5.0], [1.0, 4.0, 7.0], 4.0], 'variance': [[6.0, 6.0, 6.0], [0.6666666666666666, 0.6666666666666666, 0.6666666666666666], 6.666666666666667], 'standard deviation': [[2.449489742783178, 2.449489742783178, 2.449489742783178], [0.816496580927726, 0.816496580927726, 0.816496580927726], 2.581988897471611], 'max': [[6, 7, 8], [2, 5, 8], 8], 'min': [[0, 1, 2], [0, 3, 6], 0], 'sum': [[9, 12, 15], [3, 12, 21], 36]}
..F
======================================================================
FAIL: test_calculate_with_few_digits (test_module.UnitTests)
----------------------------------------------------------------------
ValueError: cannot reshape array of size 7 into shape (3,3)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/runner/DodgerblueSimpleSolidstatedrive/test_module.py", line 18, in test_calculate_with_few_digits
    self.assertRaisesRegex(ValueError, "List must contain nine numbers.", mean_var_std.calculate, [2,6,2,8,4,0,1,])
AssertionError: "List must contain nine numbers." does not match "cannot reshape array of size 7 into shape (3,3)"

----------------------------------------------------------------------
Ran 3 tests in 0.005s

FAILED (failures=1)
```

Perfect! Now the only thing left
out is to handle the - not enough
input error and then we are done!

And I made it work!

```python
{'mean': [[3.0, 4.0, 5.0], [1.0, 4.0, 7.0], 4.0], 'variance': [[6.0, 6.0, 6.0], [0.6666666666666666, 0.6666666666666666, 0.6666666666666666], 6.666666666666667], 'standard deviation': [[2.449489742783178, 2.449489742783178, 2.449489742783178], [0.816496580927726, 0.816496580927726, 0.816496580927726], 2.581988897471611], 'max': [[6, 7, 8], [2, 5, 8], 8], 'min': [[0, 1, 2], [0, 3, 6], 0], 'sum': [[9, 12, 15], [3, 12, 21], 36]}
...
----------------------------------------------------------------------
Ran 3 tests in 0.002s

OK
```

I just noticed how python shows
dots for every test. This is similar
to what I have seen in ruby with
`rspec`. Hmm. For failure it showed
something like `..F` to say two
passed and one failed. Again, 
similar to `rspec`! Nice :)

Now that I have completed the
project, I'll go paste the link
in the freecodecamp site! :D

Also, for the `ValueError`, I raised
the error when the length is
not exactly `9`, meaning, it will
raise error for more input too,
and not just less input

Cool! I submitted the project!
Now, the next project is about
analysing demographic data

https://en.wikipedia.org/wiki/Demography

So it's about populations I guess.

It's a 1994 census dataset, and
I have to analyze it using pandas!
:D

This seems to be an interesting
project! :D

Now, I'm using the REPL to try out
some stuff and then I'll write the
code in the python file

I also realized I didn't check how
code is being imported or how
code is being tested

I noticed that the import statements
are using the file names without the
`.py` extension. I didn't understand
much of the unit testing, but I
noticed the `unittest` module
and the `main` function in it
and how when it's given the
test module name, it runs all the
tests. It's called like this in
this project 

```python
from unittest import main

main(module='test_module', exit=False)
```

I didn't get the test code too,
as it had some class and references
to `self`, which I don't know about.
I need to learn OOPS stuff in
python before reading that code.

Anyways, back to the project.

```python
> df = pd.read_csv('adult.data.csv')
> df.shape
(32561, 15)
> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 32561 entries, 0 to 32560
Data columns (total 15 columns):
 #   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   age             32561 non-null  int64 
 1   workclass       32561 non-null  object
 2   fnlwgt          32561 non-null  int64 
 3   education       32561 non-null  object
 4   education-num   32561 non-null  int64 
 5   marital-status  32561 non-null  object
 6   occupation      32561 non-null  object
 7   relationship    32561 non-null  object
 8   race            32561 non-null  object
 9   sex             32561 non-null  object
 10  capital-gain    32561 non-null  int64 
 11  capital-loss    32561 non-null  int64 
 12  hours-per-week  32561 non-null  int64 
 13  native-country  32561 non-null  object
 14  salary          32561 non-null  object
dtypes: int64(6), object(9)
memory usage: 3.7+ MB
```

Okay, so we have `32561` rows and `15` columns!

Looking at the `info`, I can see that all
columns have values for all the rows, as the
non-null count is `32561` for all columns!
That's good! So I don't have to worry about
missing values or null values!

Wow, based on the size of the terminal,
it prints appropriate information!

```python
> df.describe()
                age        fnlwgt  ...  capital-loss  hours-per-week
count  32561.000000  3.256100e+04  ...  32561.000000    32561.000000
mean      38.581647  1.897784e+05  ...     87.303830       40.437456
std       13.640433  1.055500e+05  ...    402.960219       12.347429
min       17.000000  1.228500e+04  ...      0.000000        1.000000
25%       28.000000  1.178270e+05  ...      0.000000       40.000000
50%       37.000000  1.783560e+05  ...      0.000000       40.000000
75%       48.000000  2.370510e+05  ...      0.000000       45.000000
max       90.000000  1.484705e+06  ...   4356.000000       99.000000

[8 rows x 6 columns]
> df.describe()
                age        fnlwgt  education-num  capital-gain  capital-loss  hours-per-week
count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000    32561.000000
mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830       40.437456
std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219       12.347429
min       17.000000  1.228500e+04       1.000000      0.000000      0.000000        1.000000
25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000       40.000000
50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000       40.000000
75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000       45.000000
max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000       99.000000
```

Okay, so, now I have to answer some questions!

First question!

```
How many people of each race are represented in this dataset? This should be a Pandas series with race names as the index labels. (race column)
```

So, we need to get the different races in
the dataset and know the count of people
in each race. It should be a series - that
is a column, with race names as index

Okay! I got it right!

```python
> race_representation = df['race'].value_counts()
> race_representation
White                 27816
Black                  3124
Asian-Pac-Islander     1039
Amer-Indian-Eskimo      311
Other                   271
Name: race, dtype: int64
> type(race_representation)
<class 'pandas.core.series.Series'>
> race_representation.index
Index(['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], dtype='object')
```

So, there are lot of `White`s, fewer `Black`s,
`Asian-Pac-Islander`s, `Amer-Indian-Eskimo`s.
Then there's `Other`s

The next question is

```
What is the average age of men?
```

Now I got this one too! :D

```python
> men = df[ df['sex'] == 'Male' ]
> men.head()
   age         workclass  fnlwgt  education  education-num  ... capital-gain capital-loss hours-per-week native-country salary
0   39         State-gov   77516  Bachelors             13  ...         2174            0             40  United-States  <=50K
1   50  Self-emp-not-inc   83311  Bachelors             13  ...            0            0             13  United-States  <=50K
2   38           Private  215646    HS-grad              9  ...            0            0             40  United-States  <=50K
3   53           Private  234721       11th              7  ...            0            0             40  United-States  <=50K
7   52  Self-emp-not-inc  209642    HS-grad              9  ...            0            0             45  United-States   >50K

[5 rows x 15 columns]
> men['age'].head()
0    39
1    50
2    38
3    53
7    52
Name: age, dtype: int64
> men['age'].mean()
39.43354749885268
```

Now the answer is a bit long as there
are a lot of decimal points! The instructions
asked to "Round to the nearest tenth"

I didn't know what that meant, so I found a
video to check it out! As usual, Khan's
academy to the rescue!

https://www.youtube.com/watch?v=_MIn3zFkEcc

And pandas has a round function!

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.round.html

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html

But those are for dataframes and series.
Hmm

```python
> type(men['age'].mean())
<class 'numpy.float64'>
```

Okay! I found out what to do!

```python
> men_mean_age = men['age'].mean()
> men_mean_age.round()
39
> men_mean_age.round(1)
39.4
> men_mean_age.round(2)
39.43
```

I couldn't get proper documentation
around the paramter being passed.

```python
> help(mean_men_age.round)
Help on built-in function round:

round(...) method of numpy.float64 instance
    Not implemented (virtual attribute)
    
    Class generic exists solely to derive numpy scalars from, and p
ossesses,
    albeit unimplemented, all the attributes of the ndarray class
    so as to provide a uniform API.
    
    See also the corresponding attribute of the derived class of in
terest.

> import numpy as np
> help(np.float64.round)
Help on method_descriptor:

round(...)
    Not implemented (virtual attribute)
    
    Class generic exists solely to derive numpy scalars from, and p
ossesses,
    albeit unimplemented, all the attributes of the ndarray class
    so as to provide a uniform API.
    
    See also the corresponding attribute of the derived class of in
terest.
```

This is what I found. Still not exactly
for float64, but still

https://numpy.org/doc/stable/reference/generated/numpy.ndarray.round.html#numpy.ndarray.round
https://numpy.org/doc/stable/reference/generated/numpy.around.html#numpy.around

https://stackoverflow.com/questions/53267700/different-round-behaviour-for-python-round-on-float-and-numpy-float64

I think that makes it up for the mean age of men,
or the average age of men

I actually found another way to do things! :D

```python
> df.loc[ df['sex'] == 'Male', 'age']
0        39
1        50
2        38
3        53
7        52
         ..
32553    32
32554    53
32555    22
32557    40
32559    22
Name: age, Length: 21790, dtype: int64
> men_age = df.loc[ df['sex'] == 'Male', 'age']
> men_age.mean()
39.43354749885268
```

I need to understand the difference. Somehow
I get a feeling that this is better. Not sure
though. Gotta validate!

Found one stackoverflow regarding something
similar

https://stackoverflow.com/questions/48409128/what-is-the-difference-between-using-loc-and-using-just-square-brackets-to-filte

Will check more on this later!

Now, back to questions! Next one is

```
What is the percentage of people who have a Bachelor's degree?
```

Now, I checked two columns regarding education

```python
> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 32561 entries, 0 to 32560
Data columns (total 15 columns):
 #   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   age             32561 non-null  int64 
 1   workclass       32561 non-null  object
 2   fnlwgt          32561 non-null  int64 
 3   education       32561 non-null  object
 4   education-num   32561 non-null  int64 
 5   marital-status  32561 non-null  object
 6   occupation      32561 non-null  object
 7   relationship    32561 non-null  object
 8   race            32561 non-null  object
 9   sex             32561 non-null  object
 10  capital-gain    32561 non-null  int64 
 11  capital-loss    32561 non-null  int64 
 12  hours-per-week  32561 non-null  int64 
 13  native-country  32561 non-null  object
 14  salary          32561 non-null  object
dtypes: int64(6), object(9)
memory usage: 3.7+ MB
> df['education'].head()
0    Bachelors
1    Bachelors
2      HS-grad
3         11th
4    Bachelors
Name: education, dtype: object
> df['education-num'].head()
0    13
1    13
2     9
3     7
4    13
Name: education-num, dtype: int64
```

I wonder what's the need for two columns and if I
should consider using the number one, in case it
has any performance improvements compared to
strings, that I don't know of. I found a link
for some optimizations but it doesn't talk about
this problem

https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2

I think I'm just going to consider using the
string column as the number one, I don't know
if all the numbers are correctly mapped always,
that's something to be checked and putting the
number values in code would be based on what
I saw in the data. After verification it's good,
but still. Or I would need to verify the mapping
and then - also get the numeric value from one
of the rows, for `Bachelors` instead of me
directly typing a magic number in the code,
and then use it. Hmm. I think I won't do it
for this problem. Meh :P

```python
> df['education'].value_counts()
HS-grad         10501
Some-college     7291
Bachelors        5355
Masters          1723
Assoc-voc        1382
11th             1175
Assoc-acdm       1067
10th              933
7th-8th           646
Prof-school       576
9th               514
12th              433
Doctorate         413
5th-6th           333
1st-4th           168
Preschool          51
Name: education, dtype: int64
> df['education'].value_counts().sum()
32561
> df['education'].value_counts()['Bachelors']
5355
```

I'm thinking if there's a better way to do this.
To find the total number of people who have
a bachelors degree and then find the percentage
with the total number of people. Actually,
for the total number of people, I should just
go with `shape` and then get the rows in it.
Now, it's just a matter of counting the number
of people having a bachelors degree

```python
> df.query('education == "Bachelors"')
       age         workclass  ...  native-country salary
0       39         State-gov  ...   United-States  <=50K
1       50  Self-emp-not-inc  ...   United-States  <=50K
4       28           Private  ...            Cuba  <=50K
9       42           Private  ...   United-States   >50K
11      30         State-gov  ...           India   >50K
...    ...               ...  ...             ...    ...
32530   35                 ?  ...   United-States   >50K
32531   30                 ?  ...   United-States  <=50K
32533   54           Private  ...           Japan   >50K
32536   34           Private  ...   United-States   >50K
32538   38           Private  ...   United-States   >50K

[5355 rows x 15 columns]
```

A link about performance of `loc` vs `query`

https://stackoverflow.com/questions/49936557/pandas-dataframe-loc-vs-query-performance

```python
> df.loc[ df['education'] == 'Bachelors', 'education' ]
0        Bachelors
1        Bachelors
4        Bachelors
9        Bachelors
11       Bachelors
           ...    
32530    Bachelors
32531    Bachelors
32533    Bachelors
32536    Bachelors
32538    Bachelors
Name: education, Length: 5355, dtype: object
> df.loc[ df['education'] == 'Bachelors', 'education' ].shape
(5355,)
> df.loc[ df['education'] == 'Bachelors', 'education' ].shape[0]
5355
```

Okay then, I got my solution I guess!

```python
> bachelors_count = df.loc[ df['education'] == 'Bachelors', 'education' ].shape[0]
> total_count = df.shape[0]
> total_count
32561
> total_count
total_count
> total_count
32561
> bachelors_count * 100 / total_count
16.446055096587944
> type(bachelors_count * 100 / total_count)
<class 'float'>
> bachelors_pct = bachelors_count * 100 / total_count
> bachelors_pct
16.446055096587944
> bachelors_pct.round()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'float' object has no attribute 'round'
> round(bachelors_pct)
16
> round(bachelors_pct, 1)
16.4
```

Now to the next question!

```
What percentage of people with advanced education (Bachelors, Masters, or Doctorate) make more than 50K?
```

I could solve this in multiple ways. The way I just did
for `Bachelors` count. Like

```python
> df['salary'].head()
0    <=50K
1    <=50K
2    <=50K
3    <=50K
4    <=50K
Name: salary, dtype: object
> df['salary'].value_counts()
<=50K    24720
>50K      7841
Name: salary, dtype: int64
```

Many failed attempts

```python
> higher_education_more_salary = df.loc[ (df['education'] == 'Bachelors' || df['education'] == 'Masters' || df['education'] == 'Doctorate') && df['salary'] == '>50K' ]
  File "<stdin>", line 1
    higher_education_more_salary = df.loc[ (df['education'] == 'Bachelors' || df['education'] == 'Masters' || df['education'] == 'Doctorate') && df['salary'] == '>50K' ]
                                                                            ^
SyntaxError: invalid syntax

> higher_education_more_salary = df.loc[ (df['education'] == 'Bachelors' | df['education'] == 'Masters' | df['education'] == 'Doctorate') & d
f['salary'] == '>50K', 'salary' ]
Traceback (most recent call last):
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 274, in na_logical_op
    result = op(x, y)
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/roperator.py", line 56, in ror_
    return operator.or_(right, left)
TypeError: unsupported operand type(s) for |: 'str' and 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 288, in na_logical_op
    result = libops.scalar_binop(x, y, op)
  File "pandas/_libs/ops.pyx", line 199, in pandas._libs.ops.scalar_binop
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/roperator.py", line 56, in ror_
    return operator.or_(right, left)
TypeError: unsupported operand type(s) for |: 'bool' and 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/common.py", line 64, in new_method
    return method(self, other)
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/__init__.py", line 552, in wrapper
    res_values = logical_op(lvalues, rvalues, op)
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 366, in logical_op
    res_values = na_logical_op(lvalues, rvalues, op)
  File "/opt/virtualenvs/python3/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 297, in na_logical_op
    raise TypeError(
TypeError: Cannot perform 'ror_' with a dtyped [object] array and scalar of type [bool]
```

Finally this worked

```python
> higher_education_more_salary = df.loc[ ((df['education'] == 'Bachelors') | (df['education'] == 'Masters') | (df['education'] == 'Doctorate')) & ( df['salary'] == '>50K'), 'salary' ]

> higher_education_more_salary.shape
(3486,)
> higher_education_more_salary.shape[0]
3486
```

I was thinking of using `value_counts()` but that will count
the number of people educated with different education
without considering their salary. So that wouldn't be
useful. Another thing to use is the `query()` function.
I don't know if the `query()` function can return
just a subset of columns

There's some very very very old article about
performance

https://jakevdp.github.io/PythonDataScienceHandbook/03.12-performance-eval-and-query.html

It tells about year 2014

Now, to the next question

```
What percentage of people without advanced education make more than 50K?
```

```python
> lower_education_more_salary = df.loc[ (df['education'] != 'Bachelors'
) & (df['education'] != 'Masters') & (df['education'] != 'Doctorate') &
 ( df['salary'] == '>50K'), 'salary' ]
> lower_education_more_salary.shape[0]
4355
```

Now, to the next question

```
What is the minimum number of hours a person works per week?
```

```python
> df['hours-per-week'].head()
0    40
1    13
2    40
3    40
4    40
Name: hours-per-week, dtype: int64
> df['hours-per-week'].value_counts()
40    15217
50     2819
45     1824
60     1475
35     1297
      ...  
92        1
94        1
87        1
74        1
82        1
Name: hours-per-week, Length: 94, dtype: int64
> df['hours-per-week'].describe()
count    32561.000000
mean        40.437456
std         12.347429
min          1.000000
25%         40.000000
50%         40.000000
75%         45.000000
max         99.000000
Name: hours-per-week, dtype: float64
> df['hours-per-week'].min()
1
```

I found that there are some common things that
can be carved out!

```python
> isRich = df['salary'] == '>50K'
> hasAdvancedEducation = (df['education'] == 'Bachelors') | (df['education'] == 'Masters') | (df['education'] == 'Doctorate')
> higher_education_rich = df.loc[ hasAdvancedEducation & isRich, 'salary' ]
> lower_education_rich = df.loc[ (~hasAdvancedEducation) & isRich, 'salary' ]
```

For the next question

```
What percentage of the people who work the minimum number of hours per week have a salary of more than 50K?
```

```python
> isRich = df['salary'] == '>50K'
> min_work_hours = df['hours-per-week'].min()
> isMinimumHoursWorker = df['hours-per-week'] == min_work_hours
> num_min_workers = df.loc[ isMinimumHoursWorker & isRich, 'hours-per-week' ]
> num_min_workers.shape
(2,)
> num_min_workers
189      1
20072    1
Name: hours-per-week, dtype: int64
```

Now the next question

```
What country has the highest percentage of people that earn >50K and what is that percentage?
```

```python
> rich_people_count_per_country = df.loc[ isRich, 'native-country' ].
value_counts()
```

Initially I was thinking about how to do
`groupby` and then `size` on a series data
as the above is just a single column, a series.
Anyways, for me it was just a matter of counting
the values and then giving me the counts.
Also, series does have a groupby, but that can
be used in a different manner and for me the
above is enough

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.groupby.html

```python
> rich_people_count_per_country.head(1)
United-States    7171
Name: native-country, dtype: int64
> rich_people_count_per_country.head(1).index[0]
'United-States'
> rich_people_count_per_country.head(1)['United-States']
7171
```

Next question is

```
Identify the most popular occupation for those who earn >50K in India
```

```python
> isIndia = df['native-country'] == 'India'
> rich_people_occupation = df.loc[ isRich & isIndia, 'occupation' ]
> rich_people_occupation_count = rich_people_occupation.value_counts()
> most_popular_occupation_info = rich_people_occupation_count.head(1)
> most_popular_occupation_info.index[0]
'Prof-specialty'
> most_popular_occupation = most_popular_occupation_info.index[0]
```

I'm done with all the questions! Time to uncomment
the test code and run the tests and see if it's all
right!! :D :D

There were some bugs. I fixed the above code
accordingly. One was around undefined names,
that's okay, I fixed by defining correctly,
another was related to `hasAdvancedEducation`.

Apparently the `!hasAdvancedEducation` is wrong.

```python
Traceback (most recent call last):
  File "main.py", line 2, in <module>
    import demographic_data_analyzer
  File "/home/runner/fcc-demographic-data-analyzer/demographic_data_analyzer.py", line 32
    lower_education_rich = df.loc[ (!hasAdvancedEducation) & isRich, 'salary' ]
                                    ^
SyntaxError: invalid syntax
```

I changed it to `not hasAdvancedEducation` which
gave another error about something being
ambiguous

```python
Traceback (most recent call last):
  File "main.py", line 6, in <module>
    demographic_data_analyzer.calculate_demogra
phic_data()
  File "/home/runner/fcc-demographic-data-analy
zer/demographic_data_analyzer.py", line 32, in 
calculate_demographic_data
    lower_education_rich = df.loc[ (not hasAdva
ncedEducation) & isRich, 'salary' ]
  File "/opt/virtualenvs/python3/lib/python3.8/
site-packages/pandas/core/generic.py", line 147
8, in __nonzero__
    raise ValueError(
ValueError: The truth value of a Series is ambi
guous. Use a.empty, a.bool(), a.item(), a.any()
 or a.all().
```

Then I found out here about indexers and indexing.

https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html

particularly about boolean indexing

https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing

Where it spoke about `~` being the `not` operator. I have
fixed it for now

Weird that I wrote it down without even trying it out.
Damn me :P

I also read a bit about `query` and performance of it

https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#query-python-versus-pandas-syntax-comparison

https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#performance-of-query

I don't know if we can use variables inside the query.
I do remember the query throwing errors when I didn't
wrap value with quotes and it said something about
not being able to find a thing with that name. I think
it might be able to handle variables. Hmm

I'll try that soon once the tests pass. That is, one
thing with query, along with existing code, just without
`.loc`. Not sure if it can return just one column, or
a subset of columns. Gotta check

I ran the tests. Out of 10 tests, 5 passed, 5 failed.
Good that 5 passed! :)

Checking the errors now! Previously there was one
test setup related error which I created because of
one renaming. Fixed that alone

```python
Number of each race:
 White                 27816
Black                  3124
Asian-Pac-Islander     1039
Amer-Indian-Eskimo      311
Other                   271
Name: race, dtype: int64
Average age of men: 39.4
Percentage with Bachelors degrees: 16.4%
Percentage with higher education that earn >50K: 10.7%
Percentage without higher education that earn >50K: 13.4%
Min work time: 1 hours/week
Percentage of rich among those who work fewest hours: 0.0%
Country with highest percentage of rich: United-States
Highest percentage of rich people in country: 22.0%
Top occupations in India: Prof-specialty
.EEEE...E.
======================================================================
ERROR: test_higher_education_rich (test_module.DemographicAnalyzerTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/fcc-demographic-data-analyzer/test_module.py", line 26, in test_higher_education_rich
    self.assertAlmostEqual(actual, expected, "Expected different value for percentage with higher education that earn >50K.")
  File "/usr/lib/python3.8/unittest/case.py", line 957, in assertAlmostEqual
    if round(diff, places) == 0:
TypeError: 'str' object cannot be interpreted as an integer

======================================================================
ERROR: test_highest_earning_country (test_module.DemographicAnalyzerTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/fcc-demographic-data-analyzer/test_module.py", line 46, in test_highest_earning_country
    self.assertAlmostEqual(actual, expected, "Expected different value for highest earning country.")
  File "/usr/lib/python3.8/unittest/case.py", line 943, in assertAlmostEqual
    diff = abs(first - second)
TypeError: unsupported operand type(s) for -: 'str' and 'str'

======================================================================
ERROR: test_highest_earning_country_percentage (test_module.DemographicAnalyzerTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/fcc-demographic-data-analyzer/test_module.py", line 51, in test_highest_earning_country_percentage
    self.assertAlmostEqual(actual, expected, "Expected different value for heighest earning country percentage.")
  File "/usr/lib/python3.8/unittest/case.py", line 957, in assertAlmostEqual
    if round(diff, places) == 0:
TypeError: an integer is required (got type str)

======================================================================
ERROR: test_lower_education_rich (test_module.DemographicAnalyzerTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/fcc-demographic-data-analyzer/test_module.py", line 31, in test_lower_education_rich
    self.assertAlmostEqual(actual, expected, "Expected different value for percentage without higher education that earn >50K.")
  File "/usr/lib/python3.8/unittest/case.py", line 957, in assertAlmostEqual
    if round(diff, places) == 0:
TypeError: 'str' object cannot be interpreted as an integer

======================================================================
ERROR: test_rich_percentage (test_module.DemographicAnalyzerTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/runner/fcc-demographic-data-analyzer/test_module.py", line 41, in test_rich_percentage
    self.assertAlmostEqual(actual, expected, "Expected different value for percentage of rich among those who work fewest hours.")
  File "/usr/lib/python3.8/unittest/case.py", line 957, in assertAlmostEqual
    if round(diff, places) == 0:
TypeError: 'str' object cannot be interpreted as an integer

----------------------------------------------------------------------
Ran 10 tests in 2.627s

FAILED (errors=5)
```

I just checked the errors and I think there have
been some blunders! So I gotta fix them one
by one. I found out the expected answers in the
test.

Okay, the reason for the wrong answers were because
I understood the question wrong.

Starting with this one

```
What percentage of people with advanced education (Bachelors, Masters, or Doctorate) make more than 50K?
```

I took the total number of people for the
percentage. But, come to think of it, reading
the question again, it clearly asks, `what
percentage of people with advanced education`.
So, the total is people with advanced education.

Okay, I fixed that along with the not
advanced education question!

Now there are 3 errors!

Again, misunderstood another percentage
question

```
What percentage of the people who work the minimum number of hours per week have a salary of more than 50K?
```

Fixed that too! Two more errors!

Both of this is regarding the highest earning country.

```
What country has the highest percentage of people that earn >50K and what is that percentage?
```

I finally found the issue in my understanding. Again,
for percentage, I used the total count of people.
According to the question, they meant - given
the total count of people in each country, how much
people in each country earn more than 50K and which
country has the highest percentage 

Now I needed to group by country and get the total
count and also get the count of people earning
more than 50K. I was pretty confused about how to do
it

I finally found out about this

```python
> a = pd.Series([1,2,3,4], index=[1,2,3,4])
> b = pd.Series([1,2,3,4], index=[1,2,3,4])
> a / b
1    1.0
2    1.0
3    1.0
4    1.0
dtype: float64
> a * 100 / b
1    100.0
2    100.0
3    100.0
4    100.0
dtype: float64
> b = pd.Series([1,2,3], index=[1,2,3])
> a / b
1    1.0
2    1.0
3    1.0
4    NaN
dtype: float64
> b /a 
1    1.0
2    1.0
3    1.0
4    NaN
dtype: float64
> b = pd.Series([3, 1, 2], index=[3, 1, 2])
> a / b
1    1.0
2    1.0
3    1.0
4    NaN
dtype: float64
```

So, you see, I can divide two series or columns
and they will divide exactly based on the
index. I even saw this question

https://stackoverflow.com/questions/41726308/divide-two-pandas-series

So, all this lead to this

```python
> total_count_per_country = df['native-country'].value_counts()
> rich_people_count_per_country = df.loc[ isRich, 'native-country' 
].value_counts()
> rich_people_percentage = rich_people_count_per_country * 100 / total_count_per_country
> rich_people_percentage.max()
41.86046511627907
> rich_people_percentage.sort_values(ascending = False).head()
Iran      41.860465
France    41.379310
India     40.000000
Taiwan    39.215686
Japan     38.709677
Name: native-country, dtype: float64
> rich_people_percentage.sort_values(ascending = False).head(1)
Iran    41.860465
Name: native-country, dtype: float64
> richest_country_info = rich_people_percentage.sort_values(ascending = False).head(1)
> richest_country_info.index[0]
'Iran'
> richest_country = richest_country_info.index[0]
> richest_country
'Iran'
> richest_country_info[richest_country]
41.86046511627907
```

Yay! All the tests pass now!! :D :D

I was just thinking how much of a hassle it is
to find the max value in a column and then get that
whole row. In this case get the index value.
I found a link regarding this

https://stackoverflow.com/questions/15741759/find-maximum-value-of-a-column-and-return-the-corresponding-row-values-using-pan#15742147

But I guess that can't help with I'm dealing with
series a lot of times. Like, with value counts and all.
I need to first count the number of values, then only
the max comes into play and with the max, I need the
index value too!

Anyways, I'm going to leave it at this for now! :D
And submit it! :D :D

Next project is

https://www.freecodecamp.org/learn/data-analysis-with-python/data-analysis-with-python-projects/medical-data-visualizer

I'll do this later I guess!
